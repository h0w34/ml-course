{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrrBqt0vmzSE"
      },
      "source": [
        "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jozxHDpMmzSK"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jamulp2VmzSO"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v40T9gedmzSR"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCqpHEoQmzSY"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGd1JywBmzSb"
      },
      "source": [
        "**Define** a forward and backward pass procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fDBwBrxEmzSd"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially.\n",
        "\n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "\n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "\n",
        "        Just write a little loop.\n",
        "        \"\"\"\n",
        "        self.output = input\n",
        "\n",
        "        for layer in self.layers:\n",
        "            self.output = layer.forward(self.output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "\n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)\n",
        "            gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "\n",
        "        !!!\n",
        "\n",
        "        To ech module you need to provide the input, the module saw while forward pass,\n",
        "        it is used while computing gradients.\n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
        "        and NOT `input` to this Sequential module.\n",
        "\n",
        "        !!!\n",
        "\n",
        "        \"\"\"\n",
        "        # outer_module_grad = self.modules[-1].backward(self.modules[-2])\n",
        "        # self.gradInput = gradOutput # for input being the last layer output\n",
        "        m_num = len(self.modules)\n",
        "\n",
        "        for i, module in enumerate(reversed(self.modules[1:])):\n",
        "            gradOutput = module.backward(self.modules[m_num - i], gradOutput) # current gradOutput is for the prev layer. for the current one it's considered as gradInput\n",
        "\n",
        "        self.gradInput = self.modules[0].backward(input, self.gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sapo9VuHmzSg"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKX17bJmzSi"
      },
      "source": [
        "## 1. Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UvSOmKtkmzSm"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # self.output = ...\n",
        "        self.ouput = input@self.W.T + self.b\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = gradOutput@self.W.T\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):  # BxD -> BxM\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradb = gradOutput.sum(0)      # BxM -> 1xM\n",
        "        self.gradW = input.T@gradOutput     # DxB x BxM = DxM\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkf2TX0ImzSp"
      },
      "source": [
        "## 2. SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "ZbVJnpxtmzSr"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        np.exp(self.output) * (np.exp(self.output, axis=1, keepdims=True))**-1\n",
        "        # Your code goes here. ################################################\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = (gradOutput - (gradOutput * self.output).sum(axis=1, keepdims=True)) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AjKJLg1mzSs"
      },
      "source": [
        "## 3. LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "8RMN3r58mzSv"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        self.output = self.output - np.log(np.sum(np.exp(self.output)), axis=1, keepdims=True)\n",
        "        # Your code goes here. ################################################\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        #self.gradInput = self.output - gradOutput\n",
        "        if self.output == None: self.updateOutput(input)\n",
        "        self.gradInput = self.output - gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4EGPGfPmzSx"
      },
      "source": [
        "## 4. Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "collapsed": true,
        "id": "0r6L0op0mzSy"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # use self.EPS please\n",
        "        N = input.shape[0]\n",
        "        # training mode\n",
        "        if self.training==True:\n",
        "            self.b_mu = 1./N * np.sum(input, axis = 0, keepdims=True)\n",
        "            self.b_diff = input - self.b_mu\n",
        "            self.b_diffsq = self.b_diff**2\n",
        "            self.b_var = 1./N * np.sum(self.b_diffsq, axis=0, keepdims=True)\n",
        "            self.b_var_sqrt = np.sqrt(self.b_var + self.EPS)\n",
        "            self.b_varsqrt_inv = 1./self.b_var_sqrt\n",
        "            self.output = self.b_varsqrt_inv * self.b_diff\n",
        "            # update statistics\n",
        "\n",
        "            print(self.b_var, self.b_mu)\n",
        "            self.moving_mean = self.moving_mean * self.alpha + self.b_mu * (1. - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + self.b_var * (1. - self.alpha)\n",
        "        else:\n",
        "            # inference mode\n",
        "            self.output = (input - self.moving_mean) * 1./np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        #dx_norm == gradOutput\n",
        "        db_varsqrt_inv = (gradOutput * self.b_diff).sum(axis=0,keepdims=True)\n",
        "        db_diff1 = gradOutput * self.b_varsqrt_inv  # first graph edge that includes b_diff\n",
        "        db_var_sqrt = db_varsqrt_inv * -1./self.b_var_sqrt**2\n",
        "        db_var = 0.5 * 1. /self.b_var_sqrt\n",
        "        db_diffsq = 1./input.shape[0] * np.ones_like(input) * db_var\n",
        "        db_diff2 = 2*self.b_diff*db_diffsq  # second graph edge that includes b_diff\n",
        "        d_input1 = db_diff1+db_diff2  # also one of the edges with input\n",
        "        db_mu = -1*(db_diff1 + db_diff2).sum(axis=0)\n",
        "        d_input2 = 1/input.shape[0] * np.ones_like(input) * db_mu  # second input edge\n",
        "        self.gradInput = d_input1+d_input2  # summation of grads for two edges coming from the input\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 100  # Number of points\n",
        "mean = [2, 5]  # Mean of the distribution\n",
        "cov = [[0.2, 0], [0, 1.5]]  # Covariance matrix\n",
        "points = np.random.multivariate_normal(mean, cov, N)\n",
        "\n",
        "x,y = points[:,0], points[:,1]\n",
        "\n",
        "bm = BatchNormalization(alpha=0.01)\n",
        "bm.moving_mean = np.array([2, 5.01])\n",
        "bm.moving_variance = np.array([0.2, 122.5])\n",
        "\n",
        "points_bm = bm.updateOutput(points)\n",
        "x_bm = points_bm[:, 0]\n",
        "y_bm = points_bm[:, 1]\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))  # Adjust the figsize to your preference\n",
        "\n",
        "ax1.scatter(x, y)\n",
        "ax1.set_title(\"Input\")\n",
        "ax1.set_aspect('equal')  # Set aspect ratio to 'equal' for square shape\n",
        "\n",
        "ax2.scatter(x_bm, y_bm)\n",
        "ax2.set_title(\"BNormalized\")\n",
        "ax2.set_aspect('equal')  # Set aspect ratio to 'equal' for square shape\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "UO-2HbPSQzE1",
        "outputId": "8abfe4a6-384a-4b6e-caf7-3556ab28adc6"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.18805277 1.41356579]] [[1.99056171 4.83806166]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bm.backward(points, 1)"
      ],
      "metadata": {
        "id": "EB8EZkGqe4X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu = 1./N * np.sum(points_bm, axis = 0, keepdims=True)\n",
        "diff = points_bm - mu\n",
        "diff2 = diff**2\n",
        "var = 1./N * np.sum(diff2, axis=0, keepdims=True)\n",
        "\n",
        "mu, var\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDN2GnaUYX0f",
        "outputId": "0ecb0599-3ee9-4073-cb35-a12b9457f2b6"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[6.66133815e-16, 7.01660952e-16]]), array([[0.99397161, 0.99911078]]))"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu = 1./N * np.sum(points, axis = 0, keepdims=True)\n",
        "diff = points-mu\n",
        "diff2 = diff**2\n",
        "var = 1./N * np.sum(diff2, axis=0, keepdims=True)\n",
        "\n",
        "mu, var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAcNxkOebeCH",
        "outputId": "832a2a52-7ead-447b-999c-d525303e95ce"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.01497887, 5.21838345]]), array([[0.16488165, 1.12357745]]))"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "y14szPrfZBNs",
        "outputId": "ddb95c00-01cd-4d14-f64f-4002c780035c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BatchNormalization' object has no attribute 'db_diff'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-a8e24353dc4e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-0f3f88c8978c>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     32\u001b[0m          \u001b[0;34m-\u001b[0m \u001b[0mcomputing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0moptimizing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateGradInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccGradParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-136-a1bd9d969c29>\u001b[0m in \u001b[0;36mupdateGradInput\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdb_diffsq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdb_diff2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_diff\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdb_diffsq\u001b[0m  \u001b[0;31m# second graph edge that includes b_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0md_input1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_diff\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdb_diff2\u001b[0m  \u001b[0;31m# also one of the edges with input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdb_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_diff1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdb_diff2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0md_input2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb_mu\u001b[0m  \u001b[0;31m# second input edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BatchNormalization' object has no attribute 'db_diff'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "id": "iDXjX_CZmzSz"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzReqS7HmzS1"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSvs3PcTmzS2"
      },
      "source": [
        "## 5. Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.random.binomial(1, 0.5, 5)\n",
        "a = np.random.rand(3, 5)\n",
        "m, a*m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QdZ-lskMEwj",
        "outputId": "63e05401-f394-4c7b-802b-84cddbc44b2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 1, 1, 1]),\n",
              " array([[0.        , 0.        , 0.1166677 , 0.3657592 , 0.05224304],\n",
              "        [0.        , 0.        , 0.35098376, 0.97127281, 0.80131025],\n",
              "        [0.        , 0.        , 0.21870392, 0.19832836, 0.49826711]]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WOTMS7hlmzS4"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        if self.training == True:\n",
        "            self.mask = np.random.binomial(1, self.p, input.shape[-1])\n",
        "            self.output = self.mask*input/(1 - self.p)\n",
        "        else:\n",
        "            self.output = input\n",
        "\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):  # NxD\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradinput = self.mask*gradOutput  # NxD\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcbE3eHymzS6"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnaW8z5fmzS8"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "j3AEkPmkmzS9"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGgM5gakmzS-"
      },
      "source": [
        "## 6. Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kjXoxWpYmzTA"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = max(self.slope*input, input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.where(x > 0, 1, alpha) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LeakyReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrHPgnKFmzTC"
      },
      "source": [
        "## 7. ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "TyJPeTNEmzTD"
      },
      "outputs": [],
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.max(input, self.alpha*(np.exp(input) -1))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = input * np.exp(input) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ELU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0_zFAE0mzTE"
      },
      "source": [
        "## 8. SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "collapsed": true,
        "id": "mR6P-EnwmzTF"
      },
      "outputs": [],
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.log(1. + np.exp(input))\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        1./(1. + np.exp(input)) * np.exp(input) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nxfVAZmzTF"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGzK6pZmzTH"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "au9yhKbamzTI"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojZt3yiymzTJ"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tf1KXWOEmzTL"
      },
      "outputs": [],
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZi4fyWmzTN"
      },
      "source": [
        "## 9. Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.random.randint(1,10, size=(4,3))\n",
        "t = np.array([[1,0,0],[1,0,0],[0,1,0],[0,0,1]], dtype=bool)\n",
        "z[t] = 0\n",
        "z, t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M_cIwhB9Uv2",
        "outputId": "03169a21-6b0d-4a45-c25a-95c5a0b3f713"
      },
      "execution_count": 49,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[0, 4, 6],\n",
              "        [0, 4, 3],\n",
              "        [3, 0, 4],\n",
              "        [6, 4, 0]]),\n",
              " array([[ True, False, False],\n",
              "        [ True, False, False],\n",
              "        [False,  True, False],\n",
              "        [False, False,  True]]))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "642Nrfa7mzTP"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterionUnstable, self)\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        self.output = -np.log(input_clamp[target.dtype(bool)]).sum()\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput = self.gradInputInput[target.dtype(bool)] = -1. / input_clamp[target.dtype(bool)]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9kRWNV4mzTQ"
      },
      "source": [
        "## 10. Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "A8glT0dhmzTQ"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self)\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = -input[target.dtype(bool)].sum()\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput[target.dtype(bool)] = 1.\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R6L9UbCmzTR"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErYZRz0mzTS"
      },
      "source": [
        "### SGD optimizer with momentum\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n",
        "- `state` - dict with optimizator state (used to save accumulated gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3XXgTKZmzTU"
      },
      "outputs": [],
      "source": [
        "def sgd_momentum(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "\n",
        "    var_index = 0\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "\n",
        "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
        "\n",
        "            current_var -= old_grad\n",
        "            var_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zq6IegtmzTV"
      },
      "source": [
        "## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
        "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
        "\n",
        "Formulas for optimizer:\n",
        "\n",
        "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
        "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$\n",
        "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
        "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VGbUC-pWmzTW"
      },
      "outputs": [],
      "source": [
        "def adam_optimizer(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('m', {})  # first moment vars\n",
        "    state.setdefault('v', {})  # second moment vars\n",
        "    state.setdefault('t', 0)   # timestamp\n",
        "    state['t'] += 1\n",
        "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
        "        assert k in config, config.keys()\n",
        "\n",
        "    var_index = 0\n",
        "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            # <YOUR CODE> #######################################\n",
        "            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
        "            np.add(config['beta1'] * state['m'][state['t']-1], (1. - config['beta1']) * current_grad, out=var_first_moment)\n",
        "            np.add(config['beta2'] * state['v'][state['t']-1], (1. - config['beta2']) * current_grad * current_grad, out=var_second_moment)\n",
        "            current_var -= lr_t * var_first_moment / (np.sqrt(var_second_moment) + config['epsilon'])\n",
        "            #np.add(... , out=var_first_moment)\n",
        "            #np.add(... , out=var_second_moment)\n",
        "            #current_var -= ...\n",
        "            state['t'] += 1\n",
        "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
        "            assert var_first_moment is state['m'].get(var_index)\n",
        "            assert var_second_moment is state['v'].get(var_index)\n",
        "            var_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_gqAZSHmzTY"
      },
      "source": [
        "# Layers for advanced track homework\n",
        "You **don't need** to implement it if you are working on `homework_main-basic.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhXc16EomzTY"
      },
      "source": [
        "## 12. Conv2d [Advanced]\n",
        "- input:   **`batch_size x in_channels x h x w`**\n",
        "- output: **`batch_size x out_channels x h x w`**\n",
        "\n",
        "You should implement something like pytorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
        "\n",
        "Practical notes:\n",
        "- While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n",
        "- It may be convenient to use `skimage.util.pad` for zero-padding.\n",
        "- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension\n",
        "- Having troubles with understanding how to implement the layer?\n",
        " - Check the last year video of lecture 3 (starting from ~1:14:20)\n",
        " - May the google be with you"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.signal\n"
      ],
      "metadata": {
        "id": "jIcZtL0mwrsV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = 1+np.random.randint(5, size=(10))\n",
        "kernel = np.array([1,2,2,1])\n",
        "c = scipy.signal.correlate(image, kernel)\n",
        "c\n",
        "img = np.array(open('/content/c71125edb21d766bb0a2c5aef8de3e2e.png'))\n",
        "kern = np.random.uniform(-1, 1, size = (3, 3))\n",
        "\n",
        "kerneled scipy.signal.correlate(image, kernel)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IeTKJNw2v-6X",
        "outputId": "e50a7170-f35c-46f6-87e2-144e6229c5f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  4, 10, 16, 19, 18, 17, 16, 13, 13, 13, 11,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "feKFYWYymzTZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "0743c4cb-886c-49dd-ae25-d795884cc4fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Module' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7a6da7e5bd00>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
          ]
        }
      ],
      "source": [
        "import scipy as sp\n",
        "import scipy.signal\n",
        "import skimage\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(Conv2d, self).__init__()\n",
        "        assert kernel_size % 2 == 1, kernel_size\n",
        "\n",
        "        stdv = 1./np.sqrt(in_channels)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE ##############################\n",
        "\n",
        "        # 1. zero-pad the input array\n",
        "        # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n",
        "        # 3. add bias value\n",
        "\n",
        "        # self.output = ...\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE ##############################\n",
        "        # 1. zero-pad the gradOutput\n",
        "        # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n",
        "\n",
        "        # self.gradInput = ...\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE #############\n",
        "        # 1. zero-pad the input\n",
        "        # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n",
        "        # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n",
        "\n",
        "        # self.gradW = ...\n",
        "        # self.gradb = ...\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vumS_U1_mzTb"
      },
      "source": [
        "## 13. MaxPool2d [Advanced]\n",
        "- input:   **`batch_size x n_input_channels x h x w`**\n",
        "- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n",
        "\n",
        "You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n",
        "\n",
        "Practical notes:\n",
        "- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n",
        "- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n",
        "- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJr9EsefmzTd"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        input_h, input_w = input.shape[-2:]\n",
        "        # your may remove these asserts and implement MaxPool2d with padding\n",
        "        assert input_h % self.kernel_size == 0\n",
        "        assert input_w % self.kernel_size == 0\n",
        "\n",
        "        # YOUR CODE #############################\n",
        "        # self.output = ...\n",
        "        # self.max_indices = ...\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # YOUR CODE #############################\n",
        "        # self.gradInput = ...\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dDZxxEomzTf"
      },
      "source": [
        "### Flatten layer\n",
        "Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngRmy_XAmzTg"
      },
      "outputs": [],
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "         super(Flatten, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input.reshape(len(input), -1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput.reshape(input.shape)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KdNb1JYemzTh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}