{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrrBqt0vmzSE"
      },
      "source": [
        "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jozxHDpMmzSK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jamulp2VmzSO"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v40T9gedmzSR"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCqpHEoQmzSY"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGd1JywBmzSb"
      },
      "source": [
        "**Define** a forward and backward pass procedures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in range(10)]"
      ],
      "metadata": {
        "id": "okCy8jhyfX4E",
        "outputId": "c1a90be4-4d29-4192-c2ca-1a4aadbb5e4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "fDBwBrxEmzSd"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially.\n",
        "\n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "\n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "\n",
        "        Just write a little loop.\n",
        "        \"\"\"\n",
        "        self.output = input\n",
        "\n",
        "        for module in self.modules:\n",
        "            self.output = module.forward(self.output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "\n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)\n",
        "            gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "\n",
        "        !!!\n",
        "\n",
        "        To ech module you need to provide the input, the module saw while forward pass,\n",
        "        it is used while computing gradients.\n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
        "        and NOT `input` to this Sequential module.\n",
        "\n",
        "        !!!\n",
        "\n",
        "        \"\"\"\n",
        "        m_num = len(self.modules)-1\n",
        "        for i in range(m_num):\n",
        "            gradOutput = self.modules[m_num-i].backward(self.modules[m_num-i-1].output, gradOutput)\n",
        "\n",
        "        self.gradInput = self.modules[0].backward(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Create a random input tensor and gradient output tensor\n",
        "input_tensor = torch.randn(10, 5, requires_grad=True)\n",
        "grad_output_tensor = torch.randn(10, 3)\n",
        "\n",
        "# Create an instance of the provided Sequential container\n",
        "sequential_container = Sequential()\n",
        "\n",
        "# Create an instance of the PyTorch nn.Sequential container\n",
        "torch_sequential_container = nn.Sequential(\n",
        "    nn.Linear(5, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 3)\n",
        ")\n",
        "\n",
        "# Add modules to the provided Sequential container\n",
        "sequential_container.add(Linear(5, 10))\n",
        "sequential_container.add(ReLU())\n",
        "sequential_container.add(Linear(10, 3))\n",
        "\n",
        "# Set the weights and biases of the provided Sequential container to match the PyTorch nn.Sequential container\n",
        "for i, (module, torch_module) in enumerate(zip(sequential_container.modules, torch_sequential_container)):\n",
        "    if isinstance(module, Linear):\n",
        "        module.W = torch_module.weight.detach().numpy().T\n",
        "        module.b = torch_module.bias.detach().numpy()\n",
        "\n",
        "# Forward pass using the provided Sequential container\n",
        "sequential_output = sequential_container.updateOutput(input_tensor.detach().numpy())\n",
        "\n",
        "# Forward pass using the PyTorch nn.Sequential container\n",
        "torch_sequential_output = torch_sequential_container(input_tensor)\n",
        "\n",
        "# Set the gradient output to ones\n",
        "grad_output = torch.ones_like(torch_sequential_output)\n",
        "\n",
        "# Backward pass using the provided Sequential container\n",
        "sequential_container.backward(input_tensor.detach().numpy(), grad_output.detach().numpy())\n",
        "\n",
        "# Backward pass using the PyTorch nn.Sequential container\n",
        "torch_sequential_output.backward(grad_output)\n",
        "\n",
        "# Compare the outputs\n",
        "output_diff = np.abs(sequential_output - torch_sequential_output.detach().numpy())\n",
        "max_output_diff = np.max(output_diff)\n",
        "\n",
        "# Compare the gradients\n",
        "grad_input_diff = np.abs(sequential_container.gradInput - input_tensor.grad.numpy())\n",
        "max_grad_input_diff = np.max(grad_input_diff)\n",
        "\n",
        "# Check if the outputs and gradients are equal within a tolerance\n",
        "if max_output_diff < 1e-6 and max_grad_input_diff < 1e-6:\n",
        "    print(\"The outputs and gradients of the provided Sequential container and the PyTorch nn.Sequential container are equal.\")\n",
        "else:\n",
        "    print(\"The outputs and gradients of the provided Sequential container and the PyTorch nn.Sequential container are not equal.\")\n"
      ],
      "metadata": {
        "id": "vaYJ-Nzfo9uJ",
        "outputId": "bb29b1db-4c2a-482f-f177-ff3a85b59451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init W shape: (10, 5)\n",
            "Init W shape: (3, 10)\n",
            "input shape: (10, 5) W inside shape: (5, 10) B shape: (10,)\n",
            "Out shape: (10, 10)\n",
            "input shape: (10, 10) W inside shape: (10, 3) B shape: (3,)\n",
            "Out shape: (10, 3)\n",
            "input shape: (10, 10) gradOutput shape: (10, 3) W.shape: (10, 3)\n",
            "GradInput shape: (10, 10)\n",
            "input shape: (10, 5) gradOutput shape: (10, 10) W.shape: (5, 10)\n",
            "GradInput shape: (10, 5)\n",
            "The outputs and gradients of the provided Sequential container and the PyTorch nn.Sequential container are equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sapo9VuHmzSg"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKX17bJmzSi"
      },
      "source": [
        "## 1. Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "UvSOmKtkmzSm"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        print('Init W shape:' , self.W.shape)\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # self.output = ...\n",
        "        print('input shape:', input.shape, 'W inside shape:' , self.W.shape, 'B shape:', self.b.shape) # self.W somehow has swapped dimensions...\n",
        "        self.output = input@self.W + self.b\n",
        "        print('Out shape:', self.output.shape)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        print('input shape:', input.shape, 'gradOutput shape:', gradOutput.shape, 'W.shape:', self.W.shape)\n",
        "        self.gradInput = gradOutput@self.W.T\n",
        "        print('GradInput shape:', self.gradInput.shape)\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):  # BxD -> BxM\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradb = gradOutput.sum(0)      # BxM -> 1xM\n",
        "        self.gradW = input.T@gradOutput     # DxB x BxM = DxM\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(10, 5, requires_grad=True)\n",
        "\n",
        "# Create an instance of the provided Linear module\n",
        "linear_module = Linear(5, 3)\n",
        "\n",
        "# Create an instance of the PyTorch nn.Linear module with the same dimensions\n",
        "torch_linear_module = torch.nn.Linear(5, 3)\n",
        "\n",
        "# Set the weights and biases of the provided Linear module to match the PyTorch nn.Linear module\n",
        "linear_module.W = torch_linear_module.weight.detach().numpy().T\n",
        "linear_module.b = torch_linear_module.bias.detach().numpy()\n",
        "\n",
        "# Forward pass using the provided Linear module\n",
        "linear_output = linear_module.updateOutput(input_tensor.detach().numpy())\n",
        "\n",
        "# Forward pass using the PyTorch nn.Linear module\n",
        "torch_linear_output = torch_linear_module(input_tensor)\n",
        "\n",
        "# Set the gradient output to ones\n",
        "grad_output = torch.ones_like(torch_linear_output)\n",
        "\n",
        "# Backward pass using the provided Linear module\n",
        "linear_module.updateGradInput(input_tensor.detach().numpy(), grad_output.detach().numpy())\n",
        "linear_module.accGradParameters(input_tensor.detach().numpy(), grad_output.detach().numpy())\n",
        "\n",
        "# Backward pass using the PyTorch nn.Linear module\n",
        "torch_linear_output.backward(grad_output)\n",
        "torch_grad_input = input_tensor.grad\n",
        "torch_grad_W = torch_linear_module.weight.grad.T.detach().numpy()\n",
        "torch_grad_b = torch_linear_module.bias.grad.detach().numpy()\n",
        "\n",
        "# Compare the gradients\n",
        "grad_input_diff = np.abs(linear_module.gradInput - torch_grad_input.numpy())\n",
        "grad_W_diff = np.abs(linear_module.gradW - torch_grad_W)\n",
        "grad_b_diff = np.abs(linear_module.gradb - torch_grad_b)\n",
        "max_grad_input_diff = np.max(grad_input_diff)\n",
        "max_grad_W_diff = np.max(grad_W_diff)\n",
        "max_grad_b_diff = np.max(grad_b_diff)\n",
        "\n",
        "# Check if the gradients are equal within a tolerance\n",
        "if max_grad_input_diff < 1e-6 and max_grad_W_diff < 1e-6 and max_grad_b_diff < 1e-6:\n",
        "    print(\"The gradients of the provided Linear module and the PyTorch nn.Linear module are equal.\")\n",
        "else:\n",
        "    print(\"The gradients of the provided Linear module and the PyTorch nn.Linear module are not equal.\")\n"
      ],
      "metadata": {
        "id": "YfWcXzDUiAF7",
        "outputId": "ea42dd63-38c2-4e31-9fb5-27ea973fb195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init W shape: (3, 5)\n",
            "input shape: (10, 5) W inside shape: (5, 3) B shape: (3,)\n",
            "Out shape: (10, 3)\n",
            "input shape: (10, 5) gradOutput shape: (10, 3) W.shape: (5, 3)\n",
            "GradInput shape: (10, 5)\n",
            "The gradients of the provided Linear module and the PyTorch nn.Linear module are equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkf2TX0ImzSp"
      },
      "source": [
        "## 2. SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "collapsed": true,
        "id": "ZbVJnpxtmzSr"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        input_exp = np.exp(np.subtract(input, input.max(axis=1, keepdims=True)))\n",
        "        # print((np.exp(input).sum(axis=1)**-1).shape)\n",
        "        self.output = input_exp * input_exp.sum(axis=1, keepdims=True)**-1\n",
        "        # Your code goes here. ################################################\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = (gradOutput - (gradOutput * self.output).sum(axis=1, keepdims=True)) * self.output\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Sample input\n",
        "input_data = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Softmax using the provided implementation\n",
        "output_custom = softmax_custom.updateOutput(input_data)\n",
        "\n",
        "\n",
        "# Softmax using torch\n",
        "input_torch = torch.from_numpy(input_data).float()  # Convert to float data type\n",
        "output_torch = torch.softmax(input_torch, dim=1)\n",
        "grad_output_torch = torch.ones_like(output_torch)  # Dummy gradient for demonstration\n",
        "grad_input_torch = grad_output_torch * (output_torch * (1 - output_torch))\n",
        "\n",
        "# Compare outputs\n",
        "print(\"Custom Softmax output:\")\n",
        "print(output_custom)\n",
        "print(\"Torch Softmax output:\")\n",
        "print(output_torch.numpy())\n",
        "\n",
        "# Compare gradients\n",
        "print(\"Custom Softmax gradient:\")\n",
        "print(grad_input_custom)\n",
        "print(\"Torch Softmax gradient:\")\n",
        "print(grad_input_torch.numpy())\n"
      ],
      "metadata": {
        "id": "ivnEv1uKtr4X",
        "outputId": "002d4eef-9a7e-4200-a78b-d2136c919e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Softmax output:\n",
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.09003057 0.24472847 0.66524096]]\n",
            "Torch Softmax output:\n",
            "[[0.09003057 0.24472848 0.66524094]\n",
            " [0.09003057 0.24472848 0.66524094]]\n",
            "Custom Softmax gradient:\n",
            "[[0.90996943 0.75527153 0.33475904]\n",
            " [0.90996943 0.75527153 0.33475904]]\n",
            "Torch Softmax gradient:\n",
            "[[0.08192507 0.18483646 0.22269543]\n",
            " [0.08192507 0.18483646 0.22269543]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_data = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "output_grad=np.ones_like(input_data)\n",
        "# Convert our random data into pytorch tensors.\n",
        "torch_input = torch.tensor(input_data, requires_grad=True, dtype=float)\n",
        "torch_output_grad = torch.from_numpy(output_grad)\n",
        "\n",
        "\n",
        "softmax_custom = SoftMax()\n",
        "output_custom = softmax_custom.updateOutput(input_data)\n",
        "grad_output_custom = np.ones_like(output_custom)  # Dummy gradient for demonstration\n",
        "grad_input_custom = softmax_custom.updateGradInput(input_data, grad_output_custom)\n",
        "\n",
        "# Compute softmax (retain grad to check the intermediate gradient).\n",
        "torch_output = torch.softmax(torch_input, axis=1)\n",
        "torch_output.retain_grad()\n",
        "\n",
        "# Sum the output with the output_grad coefficients.\n",
        "# This will make the gradient torch_output.grad be equal to output_grad.\n",
        "loss = (torch_output * torch_output_grad).sum()\n",
        "loss.backward()\n",
        "assert np.allclose(torch_output.grad.detach().numpy(), output_grad)\n",
        "\n",
        "# Now that we have correct output_grad, let's check the input gradient.\n",
        "assert np.allclose(torch_input.grad.detach().numpy(), grad_input_custom)\n"
      ],
      "metadata": {
        "id": "8cFm2duJ1AJz"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AjKJLg1mzSs"
      },
      "source": [
        "## 3. LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "collapsed": true,
        "id": "8RMN3r58mzSv"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        self.output = self.output - np.log(np.sum(np.exp(self.output), axis=1, keepdims=True))\n",
        "        # Your code goes here. ################################################\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        #self.gradInput = self.output - gradOutput\n",
        "        if self.output is None: self.updateOutput(input)\n",
        "        self.gradInput = gradOutput - np.exp(self.output) * np.sum(gradOutput, axis=1, keepdims=True)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_data = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "output_grad=np.ones_like(input_data)\n",
        "# Convert our random data into pytorch tensors.\n",
        "torch_input = torch.tensor(input_data, requires_grad=True, dtype=float)\n",
        "torch_output_grad = torch.from_numpy(output_grad)\n",
        "\n",
        "\n",
        "softmax_custom = LogSoftMax()\n",
        "output_custom = softmax_custom.updateOutput(input_data)\n",
        "grad_output_custom = np.ones_like(output_custom)  # Dummy gradient for demonstration\n",
        "grad_input_custom = softmax_custom.updateGradInput(input_data, grad_output_custom)\n",
        "\n",
        "# Compute softmax (retain grad to check the intermediate gradient).\n",
        "torch_output = torch.log_softmax(torch_input, axis=1)\n",
        "torch_output.retain_grad()\n",
        "\n",
        "# Sum the output with the output_grad coefficients.\n",
        "# This will make the gradient torch_output.grad be equal to output_grad.\n",
        "loss = (torch_output * torch_output_grad).sum()\n",
        "loss.backward()\n",
        "\n",
        "print(grad_input_custom, torch_input.grad)\n",
        "\n",
        "\n",
        "assert np.allclose(torch_output.grad.detach().numpy(), output_grad)\n",
        "\n",
        "# Now that we have correct output_grad, let's check the input gradient.\n",
        "assert np.allclose(torch_input.grad.detach().numpy(), grad_input_custom)\n"
      ],
      "metadata": {
        "id": "uyV0wWXV4dsD",
        "outputId": "481afe2b-10d1-4d01-af82-39fc4633f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.72990828  0.26581459 -0.99572287]\n",
            " [ 0.72990828  0.26581459 -0.99572287]] tensor([[ 0.7299,  0.2658, -0.9957],\n",
            "        [ 0.7299,  0.2658, -0.9957]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4EGPGfPmzSx"
      },
      "source": [
        "## 4. Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "0r6L0op0mzSy"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # use self.EPS please\n",
        "        N = input.shape[0]\n",
        "        # training mode\n",
        "        if self.training==True:\n",
        "            self.b_mu = 1./N * np.sum(input, axis = 0, keepdims=True)\n",
        "            self.b_diff = input - self.b_mu\n",
        "            self.b_diffsq = self.b_diff**2\n",
        "            self.b_var = 1./N * np.sum(self.b_diffsq, axis=0, keepdims=True)\n",
        "            self.b_var_sqrt = np.sqrt(self.b_var + self.EPS)\n",
        "            self.b_varsqrt_inv = 1./self.b_var_sqrt\n",
        "            self.output = self.b_varsqrt_inv * self.b_diff\n",
        "            # update statistics\n",
        "\n",
        "            print(self.b_var, self.b_mu)\n",
        "            self.moving_mean = self.moving_mean * self.alpha + self.b_mu * (1. - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + self.b_var * (1. - self.alpha)\n",
        "        else:\n",
        "            # inference mode\n",
        "            self.output = (input - self.moving_mean) * 1./np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        #dx_norm == gradOutput\n",
        "        db_varsqrt_inv = (gradOutput * self.b_diff).sum(axis=0,keepdims=True)\n",
        "        db_diff1 = gradOutput * self.b_varsqrt_inv  # first graph edge that includes b_diff\n",
        "        db_var_sqrt = db_varsqrt_inv * -1./self.b_var_sqrt**2\n",
        "        db_var = 0.5 * 1. /self.b_var_sqrt\n",
        "        db_diffsq = 1./input.shape[0] * np.ones_like(input) * db_var\n",
        "        db_diff2 = 2*self.b_diff*db_diffsq  # second graph edge that includes b_diff\n",
        "        d_input1 = db_diff1+db_diff2  # also one of the edges with input\n",
        "        db_mu = -1*(db_diff1 + db_diff2).sum(axis=0)\n",
        "        d_input2 = 1/input.shape[0] * np.ones_like(input) * db_mu  # second input edge\n",
        "        self.gradInput = d_input1+d_input2  # summation of grads for two edges coming from the input\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def a():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    N = 100  # Number of points\n",
        "    mean = [2, 5]  # Mean of the distribution\n",
        "    cov = [[0.2, 0], [0, 1.5]]  # Covariance matrix\n",
        "    points = np.random.multivariate_normal(mean, cov, N)\n",
        "\n",
        "    x,y = points[:,0], points[:,1]\n",
        "\n",
        "    bm = BatchNormalization(alpha=0.01)\n",
        "    bm.moving_mean = np.array([2, 5.01])\n",
        "    bm.moving_variance = np.array([0.2, 122.5])\n",
        "\n",
        "    points_bm = bm.updateOutput(points)\n",
        "    x_bm = points_bm[:, 0]\n",
        "    y_bm = points_bm[:, 1]\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))  # Adjust the figsize to your preference\n",
        "\n",
        "    ax1.scatter(x, y)\n",
        "    ax1.set_title(\"Input\")\n",
        "    ax1.set_aspect('equal')  # Set aspect ratio to 'equal' for square shape\n",
        "\n",
        "    ax2.scatter(x_bm, y_bm)\n",
        "    ax2.set_title(\"BNormalized\")\n",
        "    ax2.set_aspect('equal')  # Set aspect ratio to 'equal' for square shape\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "UO-2HbPSQzE1",
        "outputId": "6942f619-42d3-451f-f580-28c47b199d42"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.14470355 1.26837181]] [[2.0128203  5.00191178]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def b():\n",
        "    bm.backward(points, 1)\n",
        "    mu = 1./N * np.sum(points_bm, axis = 0, keepdims=True)\n",
        "    diff = points_bm - mu\n",
        "    diff2 = diff**2\n",
        "    var = 1./N * np.sum(diff2, axis=0, keepdims=True)\n",
        "\n",
        "    mu, var\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDN2GnaUYX0f",
        "outputId": "22668f4c-3294-40cf-e0be-25f84da7370b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-1.78523862e-15, -2.61943245e-16]]),\n",
              " array([[0.99313675, 0.99921221]]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def c():\n",
        "    mu = 1./N * np.sum(points, axis = 0, keepdims=True)\n",
        "    diff = points-mu\n",
        "    diff2 = diff**2\n",
        "    var = 1./N * np.sum(diff2, axis=0, keepdims=True)\n",
        "\n",
        "    mu, var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAcNxkOebeCH",
        "outputId": "d8d01eaa-ed66-410d-fbf6-41ce45a8a9a0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.0128203 , 5.00191178]]), array([[0.14470355, 1.26837181]]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y14szPrfZBNs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "iDXjX_CZmzSz"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzReqS7HmzS1"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSvs3PcTmzS2"
      },
      "source": [
        "## 5. Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QdZ-lskMEwj",
        "outputId": "46a8a964-d288-408d-e5bc-c9da27014f85"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 1, 1, 0]),\n",
              " array([[0.        , 0.68730316, 0.18171111, 0.68425091, 0.        ],\n",
              "        [0.        , 0.42656155, 0.85657482, 0.86598777, 0.        ],\n",
              "        [0.        , 0.58514326, 0.2553581 , 0.67645844, 0.        ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "WOTMS7hlmzS4"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        if self.training == True:\n",
        "            self.mask = np.random.binomial(1, self.p, input.shape[-1])\n",
        "            self.output = self.mask*input/(1 - self.p)\n",
        "        else:\n",
        "            self.output = input\n",
        "\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):  # NxD\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradinput = self.mask*gradOutput  # NxD\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcbE3eHymzS6"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnaW8z5fmzS8"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "collapsed": true,
        "id": "j3AEkPmkmzS9"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGgM5gakmzS-"
      },
      "source": [
        "## 6. Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kjXoxWpYmzTA"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = max(self.slope*input, input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.where(x > 0, 1, self.slope) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LeakyReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrHPgnKFmzTC"
      },
      "source": [
        "## 7. ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "TyJPeTNEmzTD"
      },
      "outputs": [],
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.max(input, self.alpha*(np.exp(input) -1))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = input * np.exp(input) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ELU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0_zFAE0mzTE"
      },
      "source": [
        "## 8. SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "mR6P-EnwmzTF"
      },
      "outputs": [],
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.log(1. + np.exp(input))\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        1./(1. + np.exp(input)) * np.exp(input) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nxfVAZmzTF"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGzK6pZmzTH"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "au9yhKbamzTI"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojZt3yiymzTJ"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "tf1KXWOEmzTL"
      },
      "outputs": [],
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZi4fyWmzTN"
      },
      "source": [
        "## 9. Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "642Nrfa7mzTP"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        self.output = -np.sum(np.log(3input_clamp[target.astype(bool)]))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput[target.astype(bool)] = -1. / input_clamp[target.astype(bool)]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.array([1,2,3,4])\n",
        "target = []"
      ],
      "metadata": {
        "id": "0QG7A29lufe5"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9kRWNV4mzTQ"
      },
      "source": [
        "## 10. Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "collapsed": true,
        "id": "A8glT0dhmzTQ"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = -input[target.astype(bool)].sum()\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput[target.astype(bool)] = 1.\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy example\n",
        "input = np.array([0.5, 0.2, 0.3])\n",
        "target = np.array([False, True, False])\n",
        "\n",
        "criterion = ClassNLLCriterion()\n",
        "output = criterion.updateOutput(input, target)\n",
        "gradInput = criterion.updateGradInput(input, target)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "print(\"GradInput:\", gradInput)"
      ],
      "metadata": {
        "id": "KGFQPKolsrgK",
        "outputId": "ab2ab6aa-2557-4b13-fe1d-69095e63d8a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: -0.2\n",
            "GradInput: [0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R6L9UbCmzTR"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErYZRz0mzTS"
      },
      "source": [
        "### SGD optimizer with momentum\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n",
        "- `state` - dict with optimizator state (used to save accumulated gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "M3XXgTKZmzTU"
      },
      "outputs": [],
      "source": [
        "def sgd_momentum(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "\n",
        "    var_index = 0\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "\n",
        "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
        "\n",
        "            current_var -= old_grad\n",
        "            var_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zq6IegtmzTV"
      },
      "source": [
        "## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
        "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
        "\n",
        "Formulas for optimizer:\n",
        "\n",
        "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
        "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$\n",
        "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
        "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VGbUC-pWmzTW"
      },
      "outputs": [],
      "source": [
        "def adam_optimizer(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('m', {})  # first moment vars\n",
        "    state.setdefault('v', {})  # second moment vars\n",
        "    state.setdefault('t', 0)   # timestamp\n",
        "    state['t'] += 1\n",
        "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
        "        assert k in config, config.keys()\n",
        "\n",
        "    var_index = 0\n",
        "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            # <YOUR CODE> #######################################\n",
        "            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
        "            np.add(config['beta1'] * state['m'][state['t']-1], (1. - config['beta1']) * current_grad, out=var_first_moment)\n",
        "            np.add(config['beta2'] * state['v'][state['t']-1], (1. - config['beta2']) * current_grad * current_grad, out=var_second_moment)\n",
        "            current_var -= lr_t * var_first_moment / (np.sqrt(var_second_moment) + config['epsilon'])\n",
        "            #np.add(... , out=var_first_moment)\n",
        "            #np.add(... , out=var_second_moment)\n",
        "            #current_var -= ...\n",
        "            state['t'] += 1\n",
        "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
        "            assert var_first_moment is state['m'].get(var_index)\n",
        "            assert var_second_moment is state['v'].get(var_index)\n",
        "            var_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_gqAZSHmzTY"
      },
      "source": [
        "# Layers for advanced track homework\n",
        "You **don't need** to implement it if you are working on `homework_main-basic.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhXc16EomzTY"
      },
      "source": [
        "## 12. Conv2d [Advanced]\n",
        "- input:   **`batch_size x in_channels x h x w`**\n",
        "- output: **`batch_size x out_channels x h x w`**\n",
        "\n",
        "You should implement something like pytorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
        "\n",
        "Practical notes:\n",
        "- While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n",
        "- It may be convenient to use `skimage.util.pad` for zero-padding.\n",
        "- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension\n",
        "- Having troubles with understanding how to implement the layer?\n",
        " - Check the last year video of lecture 3 (starting from ~1:14:20)\n",
        " - May the google be with you"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.signal\n"
      ],
      "metadata": {
        "id": "jIcZtL0mwrsV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d():\n",
        "    conv = torch.nn.Conv2d(\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        kernel_size=3,\n",
        "        bias=False,\n",
        "        stride = 1,\n",
        "        padding_mode='zeros',\n",
        "        padding=0\n",
        "    )\n",
        "\n",
        "    xt = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], dtype=torch.float32)\n",
        "    wt = torch.tensor([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]], dtype=torch.float32)\n",
        "\n",
        "    xt.requires_grad = True\n",
        "    conv.weight = torch.nn.Parameter(w)\n",
        "    out = conv(x)\n",
        "\n",
        "    x = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
        "    w = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
        "    out2 = correlate(x, w, mode='valid')\n",
        "\n",
        "    x = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
        "    w = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
        "    out2 = correlate(x, w, mode='valid')\n",
        "    out, out2"
      ],
      "metadata": {
        "id": "hM54gkXGyT9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "feKFYWYymzTZ"
      },
      "outputs": [],
      "source": [
        "  import scipy as sp\n",
        "  import scipy.signal\n",
        "  import skimage\n",
        "\n",
        "  class Conv2d(Module):\n",
        "      def __init__(self, in_channels, out_channels, kernel_size):\n",
        "          super(Conv2d, self).__init__()\n",
        "          assert kernel_size % 2 == 1, kernel_size\n",
        "\n",
        "          stdv = 1./np.sqrt(in_channels)\n",
        "          self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size)) # out_channels = filters num\n",
        "          self.b = np.random.uniform(-stdv, stdv, size=(out_channels,)) # one bias term per filter\n",
        "          self.in_channels = in_channels\n",
        "          self.out_channels = out_channels\n",
        "          self.kernel_size = kernel_size\n",
        "\n",
        "          self.gradW = np.zeros_like(self.W)\n",
        "          self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "      def updateOutput(self, input):\n",
        "          pad_size = self.kernel_size // 2\n",
        "          # YOUR CODE ##############################\n",
        "          padded_input = np.pad(input, ((0,0),(0,0),(pad_size,pad_size), (pad_size,pad_size))) # pad only spacial dimensions\n",
        "          # print(f'input:{input.shape}, W:{self.W.shape}, padded:{padded_input.shape}')\n",
        "          # print(padded_input)\n",
        "          self.output = scipy.signal.correlate(padded_input, self.W , mode='valid') # if it was a CORRELATION func then we would NOT rotate W by 180 degrees\n",
        "          self.output = self.output+self.b\n",
        "          print(f'out:{self.output.shape}')\n",
        "          # 1. zero-pad the input array\n",
        "          # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n",
        "          # 3. add bias value\n",
        "          # self.output = ...\n",
        "\n",
        "          return self.output\n",
        "\n",
        "      def updateGradInput(self, input, gradOutput):\n",
        "          pad_size = self.kernel_size // 2\n",
        "          # YOUR CODE ##############################\n",
        "          # 1. zero-pad the gradOutput\n",
        "          padded_gradOutput = np.pad(gradOutput, pad_size)\n",
        "          # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n",
        "          self.gradInput = scipy.signal.correlate(padded_gradOutput, self.W , mode='valid')  # if it was a CORRELATION func then we WOULD rotate W by 180 degrees\n",
        "          return self.gradInput\n",
        "\n",
        "      def accGradParameters(self, input, gradOutput):\n",
        "          pad_size = self.kernel_size // 2\n",
        "          # YOUR CODE #############\n",
        "          padded_input = np.pad(input, ((0,0),(0,0),(pad_size,pad_size), (pad_size,pad_size))) # pad only spacial dimensions\n",
        "\n",
        "          self.gradW = scipy.signal.correlate(padded_input, gradOutput, mode='valid')\n",
        "          print(gradOutput.shape)\n",
        "          print(\"COMPUTING GRADBBB!\")\n",
        "          print(gradOutput, gradOutput.shape)\n",
        "          self.gradb = gradOutput.sum(axis=(2,3))\n",
        "          # 1. zero-pad the input\n",
        "          # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n",
        "          # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n",
        "          pass\n",
        "\n",
        "      def zeroGradParameters(self):\n",
        "          self.gradW.fill(0)\n",
        "          self.gradb.fill(0)\n",
        "\n",
        "      def getParameters(self):\n",
        "          return [self.W, self.b]\n",
        "\n",
        "      def getGradParameters(self):\n",
        "          return [self.gradW, self.gradb]\n",
        "\n",
        "      def __repr__(self):\n",
        "          s = self.W.shape\n",
        "          q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
        "          return q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "F115JxHW8ZXJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d():\n",
        "    toy_input = np.array([[[[1, 2, 3],\n",
        "                          [4, 5, 6],\n",
        "                          [7, 8, 9]]]])\n",
        "    toy_weights = np.array([[[[1, 5, 5],\n",
        "                          [1, 1, 6],\n",
        "                          [7, 1, 1]]]])\n",
        "\n",
        "    toy_input_torch = torch.tensor(toy_input, dtype=torch.float32)\n",
        "    toy_weight_torch =  torch.tensor(toy_weights, dtype=torch.float32)\n",
        "\n",
        "    handmade_conv = Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
        "    handmade_conv.W = toy_weights\n",
        "    handmade_output = handmade_conv.updateOutput(toy_input)\n",
        "    hg = handmade_conv.backward(toy_input, np.ones_like(handmade_output))\n",
        "\n",
        "\n",
        "    torch_conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=(3 - 1) // 2)  # Adjust padding to match output size\n",
        "    torch_conv.weight.data = toy_weight_torch\n",
        "    torch_output = torch_conv(toy_input_torch)\n",
        "    torch_output.backward(torch.ones_like(torch_output))\n",
        "    tg = torch_conv.weight.grad\n",
        "\n",
        "    #handmade_output, torch_output, handmade_conv.gradW, tg\n",
        "    handmade_conv.gradb, torch_conv.bias.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvV0ULg58HJx",
        "outputId": "2d78e11d-325b-46d1-9f44-89f3da591e29"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out:(1, 1, 3, 3)\n",
            "(1, 1, 3, 3)\n",
            "COMPUTING GRADBBB!\n",
            "[[[[1. 1. 1.]\n",
            "   [1. 1. 1.]\n",
            "   [1. 1. 1.]]]] (1, 1, 3, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[9.]]), tensor([9.]))"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vumS_U1_mzTb"
      },
      "source": [
        "## 13. MaxPool2d [Advanced]\n",
        "- input:   **`batch_size x n_input_channels x h x w`**\n",
        "- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n",
        "\n",
        "You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n",
        "\n",
        "Practical notes:\n",
        "- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n",
        "- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n",
        "- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "gJr9EsefmzTd"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.gradInput = None\n",
        "        # no stride because it woluld require a different implementation that reshaping due to overlaps\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        input_h, input_w = input.shape[-2:]\n",
        "        # your may remove these asserts and implement MaxPool2d with padding\n",
        "        #assert input_h % self.kernel_size == 0\n",
        "        #assert input_w % self.kernel_size == 0\n",
        "        # Reshape the input tensor\n",
        "        ip = input\n",
        "        hpad_size, wpad_size = 0, 0\n",
        "        # check the sizes match\n",
        "        if input_h % self.kernel_size != 0:\n",
        "            hpad_size = ((self.kernel_size - input_h % self.kernel_size) % self.kernel_size)//2\n",
        "        if input_w % self.kernel_size != 0:\n",
        "            wpad_size = ((self.kernel_size - input_w % self.kernel_size) % self.kernel_size)//2\n",
        "\n",
        "        print(hpad_size, wpad_size)\n",
        "\n",
        "        if hpad_size or hpad_size:\n",
        "            input = np.pad(input, ((0,0), (0,0), (hpad_size,hpad_size), (wpad_size,wpad_size)))\n",
        "        # new padded h and w\n",
        "        pad_input_h, pad_input_w = input.shape[-2:]\n",
        "        # reshape the input\n",
        "        input = input.reshape(input.shape[0], input.shape[1], pad_input_h // self.kernel_size, self.kernel_size, pad_input_w // self.kernel_size, self.kernel_size)\n",
        "        # Swap axes\n",
        "        input = np.swapaxes(input, 3, 4)\n",
        "        # Compute the output\n",
        "        self.output = np.amax(input, axis=(-1, -2))\n",
        "        # Store the indices of the maximum values\n",
        "        a = np.swapaxes(self.output[:,None, None], 3, 4)\n",
        "        b = a.reshape(1,1,2,2)\n",
        "        print(f\"b!!!:{b}, {b.shape}\", ip == b)\n",
        "        self.max_indices = np.amax(input, axis=(-1, -2))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # YOUR CODE #############################\n",
        "        # self.gradInput = ...\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        output_h, output_w = self.output.shape[-2:]\n",
        "        gradOutput = gradOutput.reshape(input.shape[0], input.shape[1], output_h, output_w)\n",
        "\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(input.shape[1]):\n",
        "                for m in range(output_h):\n",
        "                    for n in range(output_w):\n",
        "                        max_indices = self.max_indices[i, j, m, n]\n",
        "                        self.gradInput[i, j, m*self.kernel_size + max_indices[0], n*self.kernel_size + max_indices[1]] = gradOutput[i, j, m, n]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def e():\n",
        "    p = MaxPool2d(3)\n",
        "    out = p.updateOutput(np.arange(36).reshape(1,1,6,6))\n",
        "\n",
        "    np.arange(36).reshape(1,1,6,6), out, p.max_indices, p.gradInput"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "opQSC3DiTF3i",
        "outputId": "6692ecdd-8eca-43e0-d435-1569ec6ab97b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (1,1,6,6) (1,1,2,2) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-3910a0a709ce>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-728cf530d94a>\u001b[0m in \u001b[0;36mupdateOutput\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"b!!!:{b}, {b.shape}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,6,6) (1,1,2,2) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QKF4ZkFETOCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g():\n",
        "    k = np.ones((1,1,3,3))\n",
        "    im = img.reshape(1,1,2,3,2,3)\n",
        "    im = np.swapaxes(im, 2,3)\n",
        "    im =  np.amax(im, axis=(4, 5))\n",
        "    img, im"
      ],
      "metadata": {
        "id": "2zeuIoJCTPLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def h():\n",
        "    im = img.reshape(1,1,2,3,2,3)\n",
        "    im = np.swapaxes(im, 2,3)\n",
        "    np.amax(im, axis=(4, 5))"
      ],
      "metadata": {
        "id": "XPiie6_kTPd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dDZxxEomzTf"
      },
      "source": [
        "### Flatten layer\n",
        "Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "ngRmy_XAmzTg"
      },
      "outputs": [],
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "         super(Flatten, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input.reshape(len(input), -1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput.reshape(input.shape)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}